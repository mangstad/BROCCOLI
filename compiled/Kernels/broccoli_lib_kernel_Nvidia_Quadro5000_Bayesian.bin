//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Mon Nov  9 01:49:33 2015 (1447051773)
// Driver 304.131
//

.version 3.0
.target sm_20, texmode_independent
.address_size 32

.const .align 8 .b8 __internal_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.entry CalculateStatisticalMapsGLMBayesian(
	.param .u32 .ptr .global .align 4 CalculateStatisticalMapsGLMBayesian_param_0,
	.param .u32 .ptr .global .align 4 CalculateStatisticalMapsGLMBayesian_param_1,
	.param .u32 .ptr .global .align 4 CalculateStatisticalMapsGLMBayesian_param_2,
	.param .u32 .ptr .global .align 4 CalculateStatisticalMapsGLMBayesian_param_3,
	.param .u32 .ptr .global .align 4 CalculateStatisticalMapsGLMBayesian_param_4,
	.param .u32 .ptr .global .align 4 CalculateStatisticalMapsGLMBayesian_param_5,
	.param .u32 .ptr .const .align 4 CalculateStatisticalMapsGLMBayesian_param_6,
	.param .u32 .ptr .const .align 4 CalculateStatisticalMapsGLMBayesian_param_7,
	.param .u32 .ptr .const .align 4 CalculateStatisticalMapsGLMBayesian_param_8,
	.param .u32 .ptr .const .align 4 CalculateStatisticalMapsGLMBayesian_param_9,
	.param .u32 .ptr .const .align 4 CalculateStatisticalMapsGLMBayesian_param_10,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_11,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_12,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_13,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_14,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_15,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_16,
	.param .u32 CalculateStatisticalMapsGLMBayesian_param_17
)
{
	.local .align 8 .b8 	__local_depot0[80];
	.reg .b32 	%SP;
	.reg .f32 	%f<261>;
	.reg .f64 	%fd<720>;
	.reg .pred 	%p<128>;
	.reg .s32 	%r<591>;
	.reg .s64 	%rl<333>;


	mov.u32 	%SP, __local_depot0;
	ld.param.u32 	%r12, [CalculateStatisticalMapsGLMBayesian_param_11];
	ld.param.u32 	%r13, [CalculateStatisticalMapsGLMBayesian_param_12];
	ld.param.u32 	%r14, [CalculateStatisticalMapsGLMBayesian_param_13];
	// inline asm
	mov.u32 	%r203, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r204, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r205, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r206, %tid.x;
	// inline asm
	add.s32 	%r218, %r206, %r203;
	mad.lo.s32 	%r23, %r205, %r204, %r218;
	// inline asm
	mov.u32 	%r207, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r208, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r209, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r210, %tid.y;
	// inline asm
	add.s32 	%r219, %r210, %r207;
	mad.lo.s32 	%r28, %r209, %r208, %r219;
	// inline asm
	mov.u32 	%r211, %envreg5;
	// inline asm
	// inline asm
	mov.u32 	%r212, %ntid.z;
	// inline asm
	// inline asm
	mov.u32 	%r213, %ctaid.z;
	// inline asm
	// inline asm
	mov.u32 	%r214, %tid.z;
	// inline asm
	add.s32 	%r220, %r214, %r211;
	mad.lo.s32 	%r221, %r213, %r212, %r220;
	setp.lt.s32 	%p1, %r23, %r12;
	setp.lt.s32 	%p2, %r28, %r13;
	and.pred  	%p3, %p1, %p2;
	setp.lt.s32 	%p4, %r221, %r14;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB0_2;

	ret;

BB0_2:
	ld.param.u32 	%r534, [CalculateStatisticalMapsGLMBayesian_param_12];
	ld.param.u32 	%r547, [CalculateStatisticalMapsGLMBayesian_param_17];
	mad.lo.s32 	%r222, %r534, %r547, %r28;
	ld.param.u32 	%r530, [CalculateStatisticalMapsGLMBayesian_param_11];
	mad.lo.s32 	%r29, %r222, %r530, %r23;
	shl.b32 	%r223, %r29, 2;
	ld.param.u32 	%r513, [CalculateStatisticalMapsGLMBayesian_param_4];
	add.s32 	%r224, %r513, %r223;
	ld.global.f32 	%f101, [%r224];
	setp.neu.f32 	%p6, %f101, 0f3F800000;
	ld.param.u32 	%r507, [CalculateStatisticalMapsGLMBayesian_param_0];
	add.s32 	%r30, %r507, %r223;
	ld.param.u32 	%r535, [CalculateStatisticalMapsGLMBayesian_param_13];
	mul.lo.s32 	%r225, %r535, %r534;
	mul.lo.s32 	%r226, %r225, %r530;
	shl.b32 	%r227, %r226, 2;
	add.s32 	%r31, %r30, %r227;
	add.s32 	%r32, %r31, %r227;
	add.s32 	%r33, %r32, %r227;
	add.s32 	%r34, %r33, %r227;
	add.s32 	%r35, %r34, %r227;
	ld.param.u32 	%r508, [CalculateStatisticalMapsGLMBayesian_param_1];
	add.s32 	%r36, %r508, %r223;
	add.s32 	%r228, %r547, %r535;
	mad.lo.s32 	%r229, %r228, %r534, %r28;
	mad.lo.s32 	%r230, %r229, %r530, %r23;
	shl.b32 	%r231, %r230, 2;
	add.s32 	%r37, %r508, %r231;
	ld.param.u32 	%r509, [CalculateStatisticalMapsGLMBayesian_param_2];
	add.s32 	%r38, %r509, %r223;
	@%p6 bra 	BB0_171;

	ld.param.u32 	%r514, [CalculateStatisticalMapsGLMBayesian_param_5];
	add.s32 	%r233, %r514, %r223;
	ld.global.u32 	%r560, [%r233];
	mov.f32 	%f104, 0f3E99999A;
	mul.f32 	%f1, %f104, 0f3E99999A;
	mov.f32 	%f105, 0f41200000;
	div.full.f32 	%f106, %f105, 0f42C80000;
	ld.param.u32 	%r546, [CalculateStatisticalMapsGLMBayesian_param_16];
	cvt.rn.f32.s32 	%f2, %r546;
	mul.f32 	%f103, %f2, %f106;
	// inline asm
	{ 
	.reg .b32 	tmp1, tmp2; 
	.reg .pred 	pred1, pred2; 
	slct.f32.f32 	%f102, 0.5, -0.5, %f103; 
	add.f32 	%f102, %f102, %f103; 
	cvt.rzi.f32.f32 	%f102, %f102; 
	abs.f32 	tmp1, %f103; 
	setp.gt.f32 	pred1, tmp1, 8388608.0; 
	selp.f32 	%f102, %f103, %f102, pred1; 
	setp.lt.f32 	pred2, tmp1, 0.5; 
	cvt.rzi.f32.f32 	tmp2, %f103; 
	selp.f32 	%f102, tmp2, %f102, pred2;
	}
	// inline asm
	cvt.rzi.s32.f32 	%r40, %f102;
	ld.param.u32 	%r529, [CalculateStatisticalMapsGLMBayesian_param_11];
	mad.lo.s32 	%r234, %r28, %r529, %r23;
	shl.b32 	%r235, %r234, 2;
	ld.param.u32 	%r512, [CalculateStatisticalMapsGLMBayesian_param_3];
	add.s32 	%r236, %r512, %r235;
	ld.param.u32 	%r517, [CalculateStatisticalMapsGLMBayesian_param_6];
	ld.const.f32 	%f222, [%r517];
	ld.global.f32 	%f223, [%r236];
	fma.rn.f32 	%f249, %f222, %f223, 0f00000000;
	ld.param.u32 	%r542, [CalculateStatisticalMapsGLMBayesian_param_14];
	shl.b32 	%r237, %r542, 2;
	add.s32 	%r238, %r517, %r237;
	ld.const.f32 	%f107, [%r238];
	fma.rn.f32 	%f243, %f107, %f223, 0f00000000;
	fma.rn.f32 	%f237, %f223, %f223, 0f00000000;
	setp.gt.s32 	%p7, %r542, 1;
	@%p7 bra 	BB0_5;

	mov.f32 	%f231, 0f00000000;
	mov.f32 	%f230, %f231;
	mov.f32 	%f229, %f231;
	mov.f32 	%f228, %f231;
	mov.f32 	%f227, %f231;
	mov.f32 	%f226, %f231;
	mov.f32 	%f225, %f231;
	mov.f32 	%f224, %f231;
	bra.uni 	BB0_8;

BB0_5:
	ld.param.u32 	%r533, [CalculateStatisticalMapsGLMBayesian_param_12];
	add.s32 	%r243, %r219, %r533;
	mad.lo.s32 	%r244, %r209, %r208, %r243;
	ld.param.u32 	%r528, [CalculateStatisticalMapsGLMBayesian_param_11];
	mad.lo.s32 	%r245, %r528, %r244, %r23;
	shl.b32 	%r246, %r245, 2;
	ld.param.u32 	%r511, [CalculateStatisticalMapsGLMBayesian_param_3];
	add.s32 	%r549, %r511, %r246;
	mul.lo.s32 	%r247, %r533, %r528;
	shl.b32 	%r42, %r247, 2;
	ld.param.u32 	%r541, [CalculateStatisticalMapsGLMBayesian_param_14];
	shl.b32 	%r43, %r541, 2;
	mov.f32 	%f231, 0f00000000;
	mov.f32 	%f230, %f231;
	mov.f32 	%f229, %f231;
	mov.f32 	%f228, %f231;
	mov.f32 	%f227, %f231;
	mov.f32 	%f226, %f231;
	mov.f32 	%f225, %f231;
	mov.f32 	%f224, %f231;
	mov.u32 	%r550, 1;
	ld.param.u32 	%r548, [CalculateStatisticalMapsGLMBayesian_param_6];
	mov.f32 	%f238, %f237;
	mov.f32 	%f244, %f243;
	mov.f32 	%f250, %f249;

BB0_6:
	mov.f32 	%f9, %f223;
	mov.f32 	%f8, %f222;
	mov.u32 	%r44, %r548;
	add.s32 	%r48, %r44, 4;
	ld.const.f32 	%f21, [%r44+4];
	ld.global.f32 	%f22, [%r549];
	fma.rn.f32 	%f250, %f21, %f22, %f250;
	add.s32 	%r248, %r44, %r43;
	ld.const.f32 	%f124, [%r248+4];
	fma.rn.f32 	%f244, %f124, %f22, %f244;
	fma.rn.f32 	%f231, %f21, %f9, %f231;
	fma.rn.f32 	%f230, %f124, %f9, %f230;
	fma.rn.f32 	%f229, %f8, %f22, %f229;
	ld.const.f32 	%f125, [%r248];
	fma.rn.f32 	%f228, %f125, %f22, %f228;
	fma.rn.f32 	%f227, %f8, %f9, %f227;
	fma.rn.f32 	%f226, %f125, %f9, %f226;
	fma.rn.f32 	%f238, %f22, %f22, %f238;
	fma.rn.f32 	%f225, %f22, %f9, %f225;
	fma.rn.f32 	%f224, %f9, %f9, %f224;
	add.s32 	%r549, %r549, %r42;
	add.s32 	%r550, %r550, 1;
	ld.param.u32 	%r540, [CalculateStatisticalMapsGLMBayesian_param_14];
	setp.lt.s32 	%p8, %r550, %r540;
	mov.u32 	%r548, %r48;
	mov.f32 	%f222, %f21;
	mov.f32 	%f223, %f22;
	@%p8 bra 	BB0_6;

	mov.f32 	%f237, %f238;
	mov.f32 	%f243, %f244;
	mov.f32 	%f249, %f250;

BB0_8:
	mov.f32 	%f44, %f249;
	mov.f32 	%f43, %f243;
	mov.f32 	%f36, %f237;
	ld.param.u32 	%r521, [CalculateStatisticalMapsGLMBayesian_param_8];
	ld.const.f32 	%f254, [%r521];
	ld.const.f32 	%f46, [%r521+8];
	ld.const.f32 	%f47, [%r521+4];
	ld.const.f32 	%f48, [%r521+12];
	ld.param.u32 	%r545, [CalculateStatisticalMapsGLMBayesian_param_16];
	add.s32 	%r249, %r40, %r545;
	setp.gt.s32 	%p9, %r249, 0;
	@%p9 bra 	BB0_10;

	mov.f32 	%f260, %f129;
	mov.f32 	%f259, %f130;
	mov.u32 	%r590, 0;
	mov.u32 	%r589, %r590;
	mov.u32 	%r588, %r590;
	mov.u32 	%r587, %r590;
	mov.u32 	%r586, %r590;
	mov.u32 	%r585, %r590;
	mov.f32 	%f258, %f131;
	bra.uni 	BB0_170;

BB0_10:
	ld.param.u32 	%r543, [CalculateStatisticalMapsGLMBayesian_param_15];
	shl.b32 	%r263, %r543, 2;
	ld.param.u32 	%r519, [CalculateStatisticalMapsGLMBayesian_param_7];
	add.s32 	%r264, %r519, %r263;
	ld.const.f32 	%f49, [%r264];
	ld.const.f32 	%f50, [%r519+4];
	ld.const.f32 	%f51, [%r264+4];
	ld.param.u32 	%r539, [CalculateStatisticalMapsGLMBayesian_param_14];
	cvt.rn.f32.s32 	%f133, %r539;
	div.full.f32 	%f134, %f133, 0f40000000;
	add.f32 	%f52, %f134, 0f3C23D70A;
	mov.f64 	%fd97, 0d41DFFFFFFFC00000;
	rcp.rn.f64 	%fd1, %fd97;
	mov.f64 	%fd2, 0d7FF0000000000000;
	mov.f32 	%f232, 0f00000000;
	mul.f32 	%f53, %f232, 0f00000000;
	ld.param.u32 	%r523, [CalculateStatisticalMapsGLMBayesian_param_9];
	ld.const.f32 	%f54, [%r523+8];
	ld.param.u32 	%r525, [CalculateStatisticalMapsGLMBayesian_param_10];
	ld.const.f32 	%f55, [%r525+8];
	ld.const.f32 	%f56, [%r523+4];
	ld.const.f32 	%f57, [%r525+4];
	ld.const.f32 	%f58, [%r523+12];
	ld.const.f32 	%f59, [%r525+12];
	add.f32 	%f60, %f231, %f229;
	add.f32 	%f61, %f230, %f228;
	ld.param.u32 	%r532, [CalculateStatisticalMapsGLMBayesian_param_12];
	add.s32 	%r268, %r219, %r532;
	mad.lo.s32 	%r269, %r209, %r208, %r268;
	ld.param.u32 	%r527, [CalculateStatisticalMapsGLMBayesian_param_11];
	mad.lo.s32 	%r270, %r527, %r269, %r23;
	shl.b32 	%r271, %r270, 2;
	ld.param.u32 	%r510, [CalculateStatisticalMapsGLMBayesian_param_3];
	add.s32 	%r53, %r510, %r271;
	mov.u32 	%r590, 0;
	mov.u32 	%r589, %r590;
	mov.u32 	%r588, %r590;
	mov.u32 	%r587, %r590;
	mov.u32 	%r586, %r590;
	mov.u32 	%r585, %r590;
	mov.u32 	%r551, %r590;
	mov.f32 	%f236, %f36;
	mov.f32 	%f242, %f43;
	mov.f32 	%f248, %f44;
	mov.f32 	%f251, %f48;
	mov.f32 	%f252, %f47;
	mov.f32 	%f253, %f46;

BB0_11:
	mov.f32 	%f69, %f254;
	mov.f32 	%f68, %f253;
	mov.f32 	%f67, %f252;
	mov.f32 	%f66, %f251;
	mov.f32 	%f65, %f248;
	mov.f32 	%f64, %f242;
	mov.f32 	%f63, %f236;
	ld.param.u32 	%r518, [CalculateStatisticalMapsGLMBayesian_param_7];
	ld.const.f32 	%f137, [%r518];
	add.f32 	%f138, %f137, %f69;
	add.f32 	%f139, %f51, %f66;
	mul.f32 	%f140, %f138, %f139;
	add.f32 	%f141, %f50, %f67;
	add.f32 	%f142, %f49, %f68;
	neg.f32 	%f143, %f142;
	fma.rn.f32 	%f144, %f143, %f141, %f140;
	add.f32 	%f145, %f144, 0f3A83126F;
	div.full.f32 	%f70, %f139, %f145;
	neg.f32 	%f146, %f141;
	div.full.f32 	%f71, %f146, %f145;
	div.full.f32 	%f72, %f138, %f145;
	mul.f32 	%f147, %f70, %f65;
	div.full.f32 	%f148, %f142, %f145;
	neg.f32 	%f149, %f148;
	fma.rn.f32 	%f73, %f149, %f64, %f147;
	mul.f32 	%f150, %f72, %f64;
	fma.rn.f32 	%f74, %f71, %f65, %f150;
	mul.f32 	%f151, %f142, %f74;
	fma.rn.f32 	%f152, %f138, %f73, %f151;
	mul.f32 	%f153, %f139, %f74;
	fma.rn.f32 	%f154, %f141, %f73, %f153;
	neg.f32 	%f155, %f73;
	fma.rn.f32 	%f156, %f155, %f152, %f63;
	neg.f32 	%f157, %f74;
	fma.rn.f32 	%f158, %f157, %f154, %f156;
	fma.rn.f32 	%f75, %f158, 0f3F000000, 0f3C23D70A;
	// inline asm
	{ 
	.reg .b32 	tmp1, tmp2; 
	.reg .pred 	pred1, pred2; 
	slct.f32.f32 	%f135, 0.5, -0.5, %f52; 
	add.f32 	%f135, %f135, %f52; 
	cvt.rzi.f32.f32 	%f135, %f135; 
	abs.f32 	tmp1, %f52; 
	setp.gt.f32 	pred1, tmp1, 8388608.0; 
	selp.f32 	%f135, %f52, %f135, pred1; 
	setp.lt.f32 	pred2, tmp1, 0.5; 
	cvt.rzi.f32.f32 	tmp2, %f52; 
	selp.f32 	%f135, tmp2, %f135, pred2;
	}
	// inline asm
	cvt.rzi.s32.f32 	%r272, %f135;
	shl.b32 	%r273, %r272, 1;
	setp.gt.s32 	%p10, %r273, 0;
	@%p10 bra 	BB0_13;

	mov.f64 	%fd704, 0d0000000000000000;
	bra.uni 	BB0_52;

BB0_13:
	mov.f64 	%fd704, 0d0000000000000000;
	mov.u32 	%r552, 0;

BB0_14:
	cvt.rn.f64.s32 	%fd106, %r560;
	mul.f64 	%fd107, %fd106, 0d40D069C000000000;
	mul.f64 	%fd101, %fd107, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd100, %fd101;
	// inline asm
	neg.f64 	%fd109, %fd100;
	fma.rn.f64 	%fd110, %fd109, %fd97, %fd107;
	cvt.rzi.s32.f64 	%r277, %fd110;
	cvt.rn.f64.s32 	%fd111, %r277;
	mul.f64 	%fd105, %fd111, %fd1;
	mul.f64 	%fd112, %fd111, 0d40D069C000000000;
	mul.f64 	%fd103, %fd112, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd102, %fd103;
	// inline asm
	neg.f64 	%fd113, %fd102;
	fma.rn.f64 	%fd114, %fd113, %fd97, %fd112;
	cvt.rzi.s32.f64 	%r560, %fd114;
	cvt.rn.f64.s32 	%fd115, %r560;
	mul.f64 	%fd5, %fd115, %fd1;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r275}, %fd105; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r276, hi}, %fd105; 
	}
	// inline asm
	setp.gt.f64 	%p11, %fd105, 0d0000000000000000;
	setp.lt.f64 	%p12, %fd105, %fd2;
	and.pred  	%p13, %p11, %p12;
	mov.f64 	%fd700, %fd105;
	mov.u32 	%r554, %r276;
	mov.u32 	%r553, %r275;
	@%p13 bra 	BB0_21;

	setp.nan.f64 	%p14, %fd105, %fd105;
	@%p14 bra 	BB0_20;

	setp.eq.f64 	%p15, %fd105, 0d0000000000000000;
	@%p15 bra 	BB0_19;

	setp.eq.f64 	%p16, %fd105, %fd2;
	@%p16 bra 	BB0_27;

	mov.f64 	%fd700, 0dFFF8000000000000;
	bra.uni 	BB0_27;

BB0_19:
	neg.f64 	%fd700, %fd2;
	bra.uni 	BB0_27;

BB0_20:
	add.f64 	%fd700, %fd105, %fd105;
	bra.uni 	BB0_27;

BB0_21:
	setp.lt.u32 	%p17, %r275, 1048576;
	@%p17 bra 	BB0_23;

	mov.u32 	%r555, -1023;
	bra.uni 	BB0_24;

BB0_23:
	mov.f64 	%fd118, 0d4350000000000000;
	mul.rn.f64 	%fd117, %fd105, %fd118;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r279}, %fd117; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r280, hi}, %fd117; 
	}
	// inline asm
	mov.u32 	%r555, -1077;
	mov.u32 	%r554, %r280;
	mov.u32 	%r553, %r279;

BB0_24:
	shr.s32 	%r284, %r553, 20;
	add.s32 	%r556, %r555, %r284;
	and.b32  	%r285, %r553, -2146435073;
	or.b32  	%r283, %r285, 1072693248;
	// inline asm
	mov.b64 	%fd119, {%r554, %r283};
	// inline asm
	setp.gt.u32 	%p18, %r283, 1073127582;
	mov.f64 	%fd699, %fd119;
	@%p18 bra 	BB0_25;
	bra.uni 	BB0_26;

BB0_25:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r286, hi}, %fd119; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r287}, %fd119; 
	}
	// inline asm
	add.s32 	%r289, %r287, -1048576;
	// inline asm
	mov.b64 	%fd122, {%r286, %r289};
	// inline asm
	add.s32 	%r556, %r556, 1;
	mov.f64 	%fd699, %fd122;

BB0_26:
	add.f64 	%fd167, %fd699, 0d3FF0000000000000;
	rcp.rn.f64 	%fd168, %fd167;
	add.f64 	%fd153, %fd699, 0dBFF0000000000000;
	mul.rn.f64 	%fd169, %fd153, %fd168;
	add.f64 	%fd158, %fd169, %fd169;
	mul.rn.f64 	%fd149, %fd158, %fd158;
	mov.f64 	%fd124, 0d3EB1380B3AE80F1E;
	mov.f64 	%fd126, 0d3ED0EE258B7A8B04;
	// inline asm
	fma.rn.f64 	%fd123, %fd124, %fd149, %fd126;
	// inline asm
	mov.f64 	%fd130, 0d3EF3B2669F02676F;
	// inline asm
	fma.rn.f64 	%fd127, %fd123, %fd149, %fd130;
	// inline asm
	mov.f64 	%fd134, 0d3F1745CBA9AB0956;
	// inline asm
	fma.rn.f64 	%fd131, %fd127, %fd149, %fd134;
	// inline asm
	mov.f64 	%fd138, 0d3F3C71C72D1B5154;
	// inline asm
	fma.rn.f64 	%fd135, %fd131, %fd149, %fd138;
	// inline asm
	mov.f64 	%fd142, 0d3F624924923BE72D;
	// inline asm
	fma.rn.f64 	%fd139, %fd135, %fd149, %fd142;
	// inline asm
	mov.f64 	%fd146, 0d3F8999999999A3C4;
	// inline asm
	fma.rn.f64 	%fd143, %fd139, %fd149, %fd146;
	// inline asm
	mov.f64 	%fd150, 0d3FB5555555555554;
	// inline asm
	fma.rn.f64 	%fd147, %fd143, %fd149, %fd150;
	// inline asm
	sub.f64 	%fd170, %fd153, %fd158;
	mov.f64 	%fd171, 0d4000000000000000;
	mul.rn.f64 	%fd154, %fd171, %fd170;
	neg.f64 	%fd152, %fd158;
	// inline asm
	fma.rn.f64 	%fd151, %fd152, %fd153, %fd154;
	// inline asm
	mul.rn.f64 	%fd172, %fd168, %fd151;
	mul.rn.f64 	%fd173, %fd147, %fd149;
	mul.rn.f64 	%fd174, %fd173, %fd158;
	add.f64 	%fd175, %fd172, %fd174;
	cvt.rn.f64.s32 	%fd164, %r556;
	mov.f64 	%fd161, 0d3FE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd155, %fd164, %fd161, %fd158;
	// inline asm
	neg.s32 	%r290, %r556;
	cvt.rn.f64.s32 	%fd160, %r290;
	// inline asm
	fma.rn.f64 	%fd159, %fd160, %fd161, %fd155;
	// inline asm
	sub.f64 	%fd176, %fd159, %fd158;
	sub.f64 	%fd166, %fd175, %fd176;
	mov.f64 	%fd165, 0d3C7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd163, %fd164, %fd165, %fd166;
	// inline asm
	add.f64 	%fd700, %fd155, %fd163;

BB0_27:
	mul.f64 	%fd178, %fd700, 0dC000000000000000;
	// inline asm
	sqrt.rn.f64 	%fd177, %fd178;
	// inline asm
	mul.f64 	%fd15, %fd5, 0d401921FB54442D18;
	setp.eq.f64 	%p19, %fd15, %fd2;
	setp.eq.f64 	%p20, %fd15, 0dFFF0000000000000;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	BB0_50;

	// inline asm
	abs.f64 	%fd179, %fd15;
	// inline asm
	setp.gt.f64 	%p22, %fd179, 0d41E0000000000000;
	@%p22 bra 	BB0_30;

	mov.f64 	%fd194, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd181, %fd15, %fd194;
	// inline asm
	cvt.rni.s32.f64 	%r291, %fd181;
	// inline asm
	cvt.rn.f64.s32 	%fd195, %r291;
	neg.f64 	%fd191, %fd195;
	mov.f64 	%fd184, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd182, %fd191, %fd184, %fd15;
	// inline asm
	mov.f64 	%fd188, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd186, %fd191, %fd188, %fd182;
	// inline asm
	mov.f64 	%fd192, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd190, %fd191, %fd192, %fd186;
	// inline asm
	mov.u32 	%r559, %r291;
	mov.f64 	%fd701, %fd190;
	bra.uni 	BB0_46;

BB0_30:
	mov.b64 	 %rl1, %fd15;
	and.b64  	%rl296, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl129, %rl3, 2047;
	add.s64 	%rl130, %rl129, 4294966272;
	cvt.u32.u64 	%r76, %rl130;
	shl.b64 	%rl131, %rl1, 11;
	or.b64  	%rl4, %rl131, -9223372036854775808;
	shr.u32 	%r295, %r76, 6;
	mov.u32 	%r296, 16;
	sub.s32 	%r77, %r296, %r295;
	mov.u32 	%r297, 15;
	sub.s32 	%r557, %r297, %r295;
	mov.u32 	%r298, 19;
	sub.s32 	%r79, %r298, %r295;
	mov.u32 	%r293, 18;
	// inline asm
	min.s32 	%r292, %r293, %r79;
	// inline asm
	setp.lt.s32 	%p23, %r557, %r292;
	@%p23 bra 	BB0_32;

	mov.u64 	%rl293, 0;
	bra.uni 	BB0_34;

BB0_32:
	mov.u32 	%r299, 1;
	sub.s32 	%r80, %r299, %r77;
	mov.u64 	%rl293, 0;

BB0_33:
	.pragma "nounroll";
	shl.b32 	%r303, %r557, 3;
	mov.u32 	%r304, __internal_i2opi_d;
	add.s32 	%r305, %r304, %r303;
	ld.const.u64 	%rl135, [%r305];
	mul.lo.s64 	%rl137, %rl135, %rl4;
	// inline asm
	mul.hi.u64 	%rl134, %rl135, %rl4;
	// inline asm
	mad.lo.s64 	%rl138, %rl135, %rl4, %rl293;
	setp.lt.u64 	%p24, %rl138, %rl137;
	selp.u64 	%rl139, 1, 0, %p24;
	add.s64 	%rl293, %rl139, %rl134;
	add.s32 	%r306, %r80, %r557;
	shl.b32 	%r307, %r306, 3;
	add.u32 	%r308, %SP, 40;
	add.s32 	%r309, %r308, %r307;
	st.local.u64 	[%r309], %rl138;
	// inline asm
	min.s32 	%r300, %r293, %r79;
	// inline asm
	add.s32 	%r557, %r557, 1;
	setp.lt.s32 	%p25, %r557, %r300;
	@%p25 bra 	BB0_33;

BB0_34:
	mov.u32 	%r310, 1;
	sub.s32 	%r311, %r310, %r77;
	add.s32 	%r312, %r311, %r557;
	shl.b32 	%r313, %r312, 3;
	add.u32 	%r314, %SP, 40;
	add.s32 	%r315, %r314, %r313;
	st.local.u64 	[%r315], %rl293;
	ld.local.u64 	%rl294, [%SP+64];
	ld.local.u64 	%rl295, [%SP+56];
	and.b32  	%r316, %r76, 63;
	setp.eq.s32 	%p26, %r316, 0;
	@%p26 bra 	BB0_36;

	and.b64  	%rl140, %rl3, 63;
	cvt.u32.u64 	%r317, %rl140;
	shl.b64 	%rl141, %rl294, %r317;
	neg.s32 	%r318, %r76;
	and.b32  	%r319, %r318, 63;
	shr.u64 	%rl142, %rl295, %r319;
	or.b64  	%rl294, %rl142, %rl141;
	shl.b64 	%rl143, %rl295, %r317;
	ld.local.u64 	%rl144, [%SP+48];
	shr.u64 	%rl145, %rl144, %r319;
	or.b64  	%rl295, %rl145, %rl143;

BB0_36:
	shr.u64 	%rl146, %rl294, 62;
	cvt.u32.u64 	%r320, %rl146;
	shr.u64 	%rl147, %rl295, 62;
	shl.b64 	%rl148, %rl294, 2;
	or.b64  	%rl300, %rl147, %rl148;
	shl.b64 	%rl15, %rl295, 2;
	setp.ne.s64 	%p27, %rl15, 0;
	selp.u64 	%rl149, 1, 0, %p27;
	or.b64  	%rl150, %rl149, %rl300;
	setp.gt.u64 	%p28, %rl150, -9223372036854775808;
	selp.u32 	%r321, 1, 0, %p28;
	add.s32 	%r322, %r321, %r320;
	neg.s32 	%r323, %r322;
	setp.lt.s64 	%p29, %rl1, 0;
	selp.b32 	%r559, %r323, %r322, %p29;
	@%p28 bra 	BB0_38;

	mov.u64 	%rl299, %rl15;
	bra.uni 	BB0_39;

BB0_38:
	not.b64 	%rl151, %rl300;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p30, %rl15, 0;
	selp.u64 	%rl152, 1, 0, %p30;
	add.s64 	%rl300, %rl152, %rl151;
	xor.b64  	%rl296, %rl296, -9223372036854775808;
	mov.u64 	%rl299, %rl16;

BB0_39:
	mov.u64 	%rl298, %rl299;
	setp.gt.s64 	%p31, %rl300, 0;
	@%p31 bra 	BB0_41;

	mov.u32 	%r558, 0;
	bra.uni 	BB0_43;

BB0_41:
	mov.u32 	%r558, 0;

BB0_42:
	shr.u64 	%rl153, %rl298, 63;
	shl.b64 	%rl154, %rl300, 1;
	or.b64  	%rl300, %rl153, %rl154;
	shl.b64 	%rl298, %rl298, 1;
	add.s32 	%r558, %r558, -1;
	setp.gt.s64 	%p32, %rl300, 0;
	@%p32 bra 	BB0_42;

BB0_43:
	mul.lo.s64 	%rl302, %rl300, -3958705157555305931;
	mov.u64 	%rl157, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl155, %rl300, %rl157;
	// inline asm
	setp.gt.s64 	%p33, %rl155, 0;
	mov.u64 	%rl301, %rl155;
	@%p33 bra 	BB0_44;
	bra.uni 	BB0_45;

BB0_44:
	shl.b64 	%rl158, %rl155, 1;
	shr.u64 	%rl159, %rl302, 63;
	or.b64  	%rl301, %rl158, %rl159;
	mul.lo.s64 	%rl302, %rl300, -7917410315110611862;
	add.s32 	%r558, %r558, -1;

BB0_45:
	setp.ne.s64 	%p34, %rl302, 0;
	selp.u64 	%rl160, 1, 0, %p34;
	add.s64 	%rl161, %rl160, %rl301;
	add.s32 	%r326, %r558, 1022;
	cvt.u64.u32 	%rl162, %r326;
	shl.b64 	%rl163, %rl162, 52;
	shr.u64 	%rl164, %rl161, 11;
	shr.u64 	%rl165, %rl161, 10;
	and.b64  	%rl166, %rl165, 1;
	add.s64 	%rl167, %rl163, %rl164;
	add.s64 	%rl168, %rl167, %rl166;
	or.b64  	%rl169, %rl168, %rl296;
	mov.b64 	 %fd701, %rl169;

BB0_46:
	add.s32 	%r91, %r559, 1;
	and.b32  	%r327, %r91, 1;
	setp.eq.s32 	%p35, %r327, 0;
	mul.rn.f64 	%fd19, %fd701, %fd701;
	@%p35 bra 	BB0_48;

	mov.f64 	%fd197, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd199, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd196, %fd197, %fd19, %fd199;
	// inline asm
	mov.f64 	%fd203, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd200, %fd196, %fd19, %fd203;
	// inline asm
	mov.f64 	%fd207, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd204, %fd200, %fd19, %fd207;
	// inline asm
	mov.f64 	%fd211, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd208, %fd204, %fd19, %fd211;
	// inline asm
	mov.f64 	%fd215, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd212, %fd208, %fd19, %fd215;
	// inline asm
	mov.f64 	%fd219, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd216, %fd212, %fd19, %fd219;
	// inline asm
	mov.f64 	%fd223, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd220, %fd216, %fd19, %fd223;
	// inline asm
	mov.f64 	%fd702, %fd220;
	bra.uni 	BB0_49;

BB0_48:
	mov.f64 	%fd225, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd227, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd224, %fd225, %fd19, %fd227;
	// inline asm
	mov.f64 	%fd231, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd228, %fd224, %fd19, %fd231;
	// inline asm
	mov.f64 	%fd235, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd232, %fd228, %fd19, %fd235;
	// inline asm
	mov.f64 	%fd239, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd236, %fd232, %fd19, %fd239;
	// inline asm
	mov.f64 	%fd243, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd240, %fd236, %fd19, %fd243;
	// inline asm
	mul.rn.f64 	%fd245, %fd240, %fd19;
	// inline asm
	fma.rn.f64 	%fd244, %fd245, %fd701, %fd701;
	// inline asm
	mov.f64 	%fd702, %fd244;

BB0_49:
	and.b32  	%r328, %r91, 2;
	setp.eq.s32 	%p36, %r328, 0;
	neg.f64 	%fd248, %fd702;
	selp.f64 	%fd703, %fd702, %fd248, %p36;
	bra.uni 	BB0_51;

BB0_50:
	mov.f64 	%fd703, 0dFFF8000000000000;

BB0_51:
	mul.f64 	%fd249, %fd177, %fd703;
	fma.rn.f64 	%fd704, %fd249, %fd249, %fd704;
	// inline asm
	{ 
	.reg .b32 	tmp1, tmp2; 
	.reg .pred 	pred1, pred2; 
	slct.f32.f32 	%f159, 0.5, -0.5, %f52; 
	add.f32 	%f159, %f159, %f52; 
	cvt.rzi.f32.f32 	%f159, %f159; 
	abs.f32 	tmp1, %f52; 
	setp.gt.f32 	pred1, tmp1, 8388608.0; 
	selp.f32 	%f159, %f52, %f159, pred1; 
	setp.lt.f32 	pred2, tmp1, 0.5; 
	cvt.rzi.f32.f32 	tmp2, %f52; 
	selp.f32 	%f159, tmp2, %f159, pred2;
	}
	// inline asm
	cvt.rzi.s32.f32 	%r329, %f159;
	shl.b32 	%r330, %r329, 1;
	add.s32 	%r552, %r552, 1;
	setp.lt.s32 	%p37, %r552, %r330;
	@%p37 bra 	BB0_14;

BB0_52:
	cvt.f64.f32 	%fd256, %f75;
	add.f64 	%fd257, %fd256, %fd256;
	div.rn.f64 	%fd258, %fd257, %fd704;
	cvt.rn.f32.f64 	%f76, %fd258;
	cvt.rn.f64.s32 	%fd259, %r560;
	mul.f64 	%fd260, %fd259, 0d40D069C000000000;
	mul.f64 	%fd251, %fd260, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd250, %fd251;
	// inline asm
	neg.f64 	%fd262, %fd250;
	fma.rn.f64 	%fd263, %fd262, %fd97, %fd260;
	cvt.rzi.s32.f64 	%r333, %fd263;
	cvt.rn.f64.s32 	%fd264, %r333;
	mul.f64 	%fd255, %fd264, %fd1;
	mul.f64 	%fd265, %fd264, 0d40D069C000000000;
	mul.f64 	%fd253, %fd265, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd252, %fd253;
	// inline asm
	neg.f64 	%fd266, %fd252;
	fma.rn.f64 	%fd267, %fd266, %fd97, %fd265;
	cvt.rzi.s32.f64 	%r334, %fd267;
	cvt.rn.f64.s32 	%fd29, %r334;
	mul.f64 	%fd30, %fd29, %fd1;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r331}, %fd255; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r332, hi}, %fd255; 
	}
	// inline asm
	setp.gt.f64 	%p38, %fd255, 0d0000000000000000;
	setp.lt.f64 	%p39, %fd255, %fd2;
	and.pred  	%p40, %p38, %p39;
	mov.f64 	%fd706, %fd255;
	mov.u32 	%r562, %r332;
	mov.u32 	%r561, %r331;
	@%p40 bra 	BB0_59;

	setp.nan.f64 	%p41, %fd255, %fd255;
	@%p41 bra 	BB0_58;

	setp.eq.f64 	%p42, %fd255, 0d0000000000000000;
	@%p42 bra 	BB0_57;

	setp.eq.f64 	%p43, %fd255, %fd2;
	@%p43 bra 	BB0_65;

	mov.f64 	%fd706, 0dFFF8000000000000;
	bra.uni 	BB0_65;

BB0_57:
	neg.f64 	%fd706, %fd2;
	bra.uni 	BB0_65;

BB0_58:
	add.f64 	%fd706, %fd255, %fd255;
	bra.uni 	BB0_65;

BB0_59:
	setp.lt.u32 	%p44, %r331, 1048576;
	@%p44 bra 	BB0_61;

	mov.u32 	%r563, -1023;
	bra.uni 	BB0_62;

BB0_61:
	mov.f64 	%fd270, 0d4350000000000000;
	mul.rn.f64 	%fd269, %fd255, %fd270;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r336}, %fd269; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r337, hi}, %fd269; 
	}
	// inline asm
	mov.u32 	%r563, -1077;
	mov.u32 	%r562, %r337;
	mov.u32 	%r561, %r336;

BB0_62:
	shr.s32 	%r341, %r561, 20;
	add.s32 	%r564, %r563, %r341;
	and.b32  	%r342, %r561, -2146435073;
	or.b32  	%r340, %r342, 1072693248;
	// inline asm
	mov.b64 	%fd271, {%r562, %r340};
	// inline asm
	setp.gt.u32 	%p45, %r340, 1073127582;
	mov.f64 	%fd705, %fd271;
	@%p45 bra 	BB0_63;
	bra.uni 	BB0_64;

BB0_63:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r343, hi}, %fd271; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r344}, %fd271; 
	}
	// inline asm
	add.s32 	%r346, %r344, -1048576;
	// inline asm
	mov.b64 	%fd274, {%r343, %r346};
	// inline asm
	add.s32 	%r564, %r564, 1;
	mov.f64 	%fd705, %fd274;

BB0_64:
	add.f64 	%fd319, %fd705, 0d3FF0000000000000;
	rcp.rn.f64 	%fd320, %fd319;
	add.f64 	%fd305, %fd705, 0dBFF0000000000000;
	mul.rn.f64 	%fd321, %fd305, %fd320;
	add.f64 	%fd310, %fd321, %fd321;
	mul.rn.f64 	%fd301, %fd310, %fd310;
	mov.f64 	%fd276, 0d3EB1380B3AE80F1E;
	mov.f64 	%fd278, 0d3ED0EE258B7A8B04;
	// inline asm
	fma.rn.f64 	%fd275, %fd276, %fd301, %fd278;
	// inline asm
	mov.f64 	%fd282, 0d3EF3B2669F02676F;
	// inline asm
	fma.rn.f64 	%fd279, %fd275, %fd301, %fd282;
	// inline asm
	mov.f64 	%fd286, 0d3F1745CBA9AB0956;
	// inline asm
	fma.rn.f64 	%fd283, %fd279, %fd301, %fd286;
	// inline asm
	mov.f64 	%fd290, 0d3F3C71C72D1B5154;
	// inline asm
	fma.rn.f64 	%fd287, %fd283, %fd301, %fd290;
	// inline asm
	mov.f64 	%fd294, 0d3F624924923BE72D;
	// inline asm
	fma.rn.f64 	%fd291, %fd287, %fd301, %fd294;
	// inline asm
	mov.f64 	%fd298, 0d3F8999999999A3C4;
	// inline asm
	fma.rn.f64 	%fd295, %fd291, %fd301, %fd298;
	// inline asm
	mov.f64 	%fd302, 0d3FB5555555555554;
	// inline asm
	fma.rn.f64 	%fd299, %fd295, %fd301, %fd302;
	// inline asm
	sub.f64 	%fd322, %fd305, %fd310;
	mov.f64 	%fd323, 0d4000000000000000;
	mul.rn.f64 	%fd306, %fd323, %fd322;
	neg.f64 	%fd304, %fd310;
	// inline asm
	fma.rn.f64 	%fd303, %fd304, %fd305, %fd306;
	// inline asm
	mul.rn.f64 	%fd324, %fd320, %fd303;
	mul.rn.f64 	%fd325, %fd299, %fd301;
	mul.rn.f64 	%fd326, %fd325, %fd310;
	add.f64 	%fd327, %fd324, %fd326;
	cvt.rn.f64.s32 	%fd316, %r564;
	mov.f64 	%fd313, 0d3FE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd307, %fd316, %fd313, %fd310;
	// inline asm
	neg.s32 	%r347, %r564;
	cvt.rn.f64.s32 	%fd312, %r347;
	// inline asm
	fma.rn.f64 	%fd311, %fd312, %fd313, %fd307;
	// inline asm
	sub.f64 	%fd328, %fd311, %fd310;
	sub.f64 	%fd318, %fd327, %fd328;
	mov.f64 	%fd317, 0d3C7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd315, %fd316, %fd317, %fd318;
	// inline asm
	add.f64 	%fd706, %fd307, %fd315;

BB0_65:
	mul.f64 	%fd330, %fd706, 0dC000000000000000;
	// inline asm
	sqrt.rn.f64 	%fd329, %fd330;
	// inline asm
	mul.f64 	%fd40, %fd30, 0d401921FB54442D18;
	setp.eq.f64 	%p46, %fd40, %fd2;
	mov.f64 	%fd41, 0dFFF0000000000000;
	setp.eq.f64 	%p47, %fd40, 0dFFF0000000000000;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	BB0_88;

	// inline asm
	abs.f64 	%fd331, %fd40;
	// inline asm
	setp.gt.f64 	%p49, %fd331, 0d41E0000000000000;
	@%p49 bra 	BB0_68;

	mov.f64 	%fd346, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd333, %fd40, %fd346;
	// inline asm
	cvt.rni.s32.f64 	%r348, %fd333;
	// inline asm
	cvt.rn.f64.s32 	%fd347, %r348;
	neg.f64 	%fd343, %fd347;
	mov.f64 	%fd336, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd334, %fd343, %fd336, %fd40;
	// inline asm
	mov.f64 	%fd340, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd338, %fd343, %fd340, %fd334;
	// inline asm
	mov.f64 	%fd344, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd342, %fd343, %fd344, %fd338;
	// inline asm
	mov.u32 	%r567, %r348;
	mov.f64 	%fd707, %fd342;
	bra.uni 	BB0_84;

BB0_68:
	mov.b64 	 %rl33, %fd40;
	and.b64  	%rl306, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl170, %rl35, 2047;
	add.s64 	%rl171, %rl170, 4294966272;
	cvt.u32.u64 	%r105, %rl171;
	shl.b64 	%rl172, %rl33, 11;
	or.b64  	%rl36, %rl172, -9223372036854775808;
	shr.u32 	%r352, %r105, 6;
	mov.u32 	%r353, 16;
	sub.s32 	%r106, %r353, %r352;
	mov.u32 	%r354, 15;
	sub.s32 	%r565, %r354, %r352;
	mov.u32 	%r355, 19;
	sub.s32 	%r108, %r355, %r352;
	mov.u32 	%r350, 18;
	// inline asm
	min.s32 	%r349, %r350, %r108;
	// inline asm
	setp.lt.s32 	%p50, %r565, %r349;
	@%p50 bra 	BB0_70;

	mov.u64 	%rl303, 0;
	bra.uni 	BB0_72;

BB0_70:
	mov.u32 	%r356, 1;
	sub.s32 	%r109, %r356, %r106;
	mov.u64 	%rl303, 0;

BB0_71:
	.pragma "nounroll";
	shl.b32 	%r360, %r565, 3;
	mov.u32 	%r361, __internal_i2opi_d;
	add.s32 	%r362, %r361, %r360;
	ld.const.u64 	%rl176, [%r362];
	mul.lo.s64 	%rl178, %rl176, %rl36;
	// inline asm
	mul.hi.u64 	%rl175, %rl176, %rl36;
	// inline asm
	mad.lo.s64 	%rl179, %rl176, %rl36, %rl303;
	setp.lt.u64 	%p51, %rl179, %rl178;
	selp.u64 	%rl180, 1, 0, %p51;
	add.s64 	%rl303, %rl180, %rl175;
	add.s32 	%r363, %r109, %r565;
	shl.b32 	%r364, %r363, 3;
	add.u32 	%r365, %SP, 0;
	add.s32 	%r366, %r365, %r364;
	st.local.u64 	[%r366], %rl179;
	// inline asm
	min.s32 	%r357, %r350, %r108;
	// inline asm
	add.s32 	%r565, %r565, 1;
	setp.lt.s32 	%p52, %r565, %r357;
	@%p52 bra 	BB0_71;

BB0_72:
	mov.u32 	%r367, 1;
	sub.s32 	%r368, %r367, %r106;
	add.s32 	%r369, %r368, %r565;
	shl.b32 	%r370, %r369, 3;
	add.u32 	%r371, %SP, 0;
	add.s32 	%r372, %r371, %r370;
	st.local.u64 	[%r372], %rl303;
	ld.local.u64 	%rl304, [%SP+24];
	ld.local.u64 	%rl305, [%SP+16];
	and.b32  	%r373, %r105, 63;
	setp.eq.s32 	%p53, %r373, 0;
	@%p53 bra 	BB0_74;

	and.b64  	%rl181, %rl35, 63;
	cvt.u32.u64 	%r374, %rl181;
	shl.b64 	%rl182, %rl304, %r374;
	neg.s32 	%r375, %r105;
	and.b32  	%r376, %r375, 63;
	shr.u64 	%rl183, %rl305, %r376;
	or.b64  	%rl304, %rl183, %rl182;
	shl.b64 	%rl184, %rl305, %r374;
	ld.local.u64 	%rl185, [%SP+8];
	shr.u64 	%rl186, %rl185, %r376;
	or.b64  	%rl305, %rl186, %rl184;

BB0_74:
	shr.u64 	%rl187, %rl304, 62;
	cvt.u32.u64 	%r377, %rl187;
	shr.u64 	%rl188, %rl305, 62;
	shl.b64 	%rl189, %rl304, 2;
	or.b64  	%rl310, %rl188, %rl189;
	shl.b64 	%rl47, %rl305, 2;
	setp.ne.s64 	%p54, %rl47, 0;
	selp.u64 	%rl190, 1, 0, %p54;
	or.b64  	%rl191, %rl190, %rl310;
	setp.gt.u64 	%p55, %rl191, -9223372036854775808;
	selp.u32 	%r378, 1, 0, %p55;
	add.s32 	%r379, %r378, %r377;
	neg.s32 	%r380, %r379;
	setp.lt.s64 	%p56, %rl33, 0;
	selp.b32 	%r567, %r380, %r379, %p56;
	@%p55 bra 	BB0_76;

	mov.u64 	%rl309, %rl47;
	bra.uni 	BB0_77;

BB0_76:
	not.b64 	%rl192, %rl310;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p57, %rl47, 0;
	selp.u64 	%rl193, 1, 0, %p57;
	add.s64 	%rl310, %rl193, %rl192;
	xor.b64  	%rl306, %rl306, -9223372036854775808;
	mov.u64 	%rl309, %rl48;

BB0_77:
	mov.u64 	%rl308, %rl309;
	setp.gt.s64 	%p58, %rl310, 0;
	@%p58 bra 	BB0_79;

	mov.u32 	%r566, 0;
	bra.uni 	BB0_81;

BB0_79:
	mov.u32 	%r566, 0;

BB0_80:
	shr.u64 	%rl194, %rl308, 63;
	shl.b64 	%rl195, %rl310, 1;
	or.b64  	%rl310, %rl194, %rl195;
	shl.b64 	%rl308, %rl308, 1;
	add.s32 	%r566, %r566, -1;
	setp.gt.s64 	%p59, %rl310, 0;
	@%p59 bra 	BB0_80;

BB0_81:
	mul.lo.s64 	%rl312, %rl310, -3958705157555305931;
	mov.u64 	%rl198, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl196, %rl310, %rl198;
	// inline asm
	setp.gt.s64 	%p60, %rl196, 0;
	mov.u64 	%rl311, %rl196;
	@%p60 bra 	BB0_82;
	bra.uni 	BB0_83;

BB0_82:
	shl.b64 	%rl199, %rl196, 1;
	shr.u64 	%rl200, %rl312, 63;
	or.b64  	%rl311, %rl199, %rl200;
	mul.lo.s64 	%rl312, %rl310, -7917410315110611862;
	add.s32 	%r566, %r566, -1;

BB0_83:
	setp.ne.s64 	%p61, %rl312, 0;
	selp.u64 	%rl201, 1, 0, %p61;
	add.s64 	%rl202, %rl201, %rl311;
	add.s32 	%r383, %r566, 1022;
	cvt.u64.u32 	%rl203, %r383;
	shl.b64 	%rl204, %rl203, 52;
	shr.u64 	%rl205, %rl202, 11;
	shr.u64 	%rl206, %rl202, 10;
	and.b64  	%rl207, %rl206, 1;
	add.s64 	%rl208, %rl204, %rl205;
	add.s64 	%rl209, %rl208, %rl207;
	or.b64  	%rl210, %rl209, %rl306;
	mov.b64 	 %fd707, %rl210;

BB0_84:
	add.s32 	%r120, %r567, 1;
	and.b32  	%r384, %r120, 1;
	setp.eq.s32 	%p62, %r384, 0;
	mul.rn.f64 	%fd45, %fd707, %fd707;
	@%p62 bra 	BB0_86;

	mov.f64 	%fd349, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd351, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd348, %fd349, %fd45, %fd351;
	// inline asm
	mov.f64 	%fd355, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd352, %fd348, %fd45, %fd355;
	// inline asm
	mov.f64 	%fd359, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd356, %fd352, %fd45, %fd359;
	// inline asm
	mov.f64 	%fd363, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd360, %fd356, %fd45, %fd363;
	// inline asm
	mov.f64 	%fd367, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd364, %fd360, %fd45, %fd367;
	// inline asm
	mov.f64 	%fd371, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd368, %fd364, %fd45, %fd371;
	// inline asm
	mov.f64 	%fd375, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd372, %fd368, %fd45, %fd375;
	// inline asm
	mov.f64 	%fd708, %fd372;
	bra.uni 	BB0_87;

BB0_86:
	mov.f64 	%fd377, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd379, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd376, %fd377, %fd45, %fd379;
	// inline asm
	mov.f64 	%fd383, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd380, %fd376, %fd45, %fd383;
	// inline asm
	mov.f64 	%fd387, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd384, %fd380, %fd45, %fd387;
	// inline asm
	mov.f64 	%fd391, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd388, %fd384, %fd45, %fd391;
	// inline asm
	mov.f64 	%fd395, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd392, %fd388, %fd45, %fd395;
	// inline asm
	mul.rn.f64 	%fd397, %fd392, %fd45;
	// inline asm
	fma.rn.f64 	%fd396, %fd397, %fd707, %fd707;
	// inline asm
	mov.f64 	%fd708, %fd396;

BB0_87:
	and.b32  	%r385, %r120, 2;
	setp.eq.s32 	%p63, %r385, 0;
	neg.f64 	%fd400, %fd708;
	selp.f64 	%fd709, %fd708, %fd400, %p63;
	bra.uni 	BB0_89;

BB0_88:
	mov.f64 	%fd709, 0dFFF8000000000000;

BB0_89:
	mul.f64 	%fd407, %fd329, %fd709;
	cvt.rn.f32.f64 	%f77, %fd407;
	mul.f64 	%fd408, %fd29, 0d40D069C000000000;
	mul.f64 	%fd402, %fd408, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd401, %fd402;
	// inline asm
	neg.f64 	%fd410, %fd401;
	fma.rn.f64 	%fd411, %fd410, %fd97, %fd408;
	cvt.rzi.s32.f64 	%r388, %fd411;
	cvt.rn.f64.s32 	%fd412, %r388;
	mul.f64 	%fd406, %fd412, %fd1;
	mul.f64 	%fd413, %fd412, 0d40D069C000000000;
	mul.f64 	%fd404, %fd413, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd403, %fd404;
	// inline asm
	neg.f64 	%fd414, %fd403;
	fma.rn.f64 	%fd415, %fd414, %fd97, %fd413;
	cvt.rzi.s32.f64 	%r389, %fd415;
	cvt.rn.f64.s32 	%fd53, %r389;
	mul.f64 	%fd54, %fd53, %fd1;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r386}, %fd406; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r387, hi}, %fd406; 
	}
	// inline asm
	setp.gt.f64 	%p64, %fd406, 0d0000000000000000;
	setp.lt.f64 	%p65, %fd406, %fd2;
	and.pred  	%p66, %p64, %p65;
	mov.f64 	%fd711, %fd406;
	mov.u32 	%r569, %r387;
	mov.u32 	%r568, %r386;
	@%p66 bra 	BB0_96;

	setp.nan.f64 	%p67, %fd406, %fd406;
	@%p67 bra 	BB0_95;

	setp.eq.f64 	%p68, %fd406, 0d0000000000000000;
	@%p68 bra 	BB0_94;

	setp.eq.f64 	%p69, %fd406, %fd2;
	@%p69 bra 	BB0_102;

	mov.f64 	%fd711, 0dFFF8000000000000;
	bra.uni 	BB0_102;

BB0_94:
	neg.f64 	%fd711, %fd2;
	bra.uni 	BB0_102;

BB0_95:
	add.f64 	%fd711, %fd406, %fd406;
	bra.uni 	BB0_102;

BB0_96:
	setp.lt.u32 	%p70, %r386, 1048576;
	@%p70 bra 	BB0_98;

	mov.u32 	%r570, -1023;
	bra.uni 	BB0_99;

BB0_98:
	mov.f64 	%fd418, 0d4350000000000000;
	mul.rn.f64 	%fd417, %fd406, %fd418;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r391}, %fd417; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r392, hi}, %fd417; 
	}
	// inline asm
	mov.u32 	%r570, -1077;
	mov.u32 	%r569, %r392;
	mov.u32 	%r568, %r391;

BB0_99:
	shr.s32 	%r396, %r568, 20;
	add.s32 	%r571, %r570, %r396;
	and.b32  	%r397, %r568, -2146435073;
	or.b32  	%r395, %r397, 1072693248;
	// inline asm
	mov.b64 	%fd419, {%r569, %r395};
	// inline asm
	setp.gt.u32 	%p71, %r395, 1073127582;
	mov.f64 	%fd710, %fd419;
	@%p71 bra 	BB0_100;
	bra.uni 	BB0_101;

BB0_100:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r398, hi}, %fd419; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r399}, %fd419; 
	}
	// inline asm
	add.s32 	%r401, %r399, -1048576;
	// inline asm
	mov.b64 	%fd422, {%r398, %r401};
	// inline asm
	add.s32 	%r571, %r571, 1;
	mov.f64 	%fd710, %fd422;

BB0_101:
	add.f64 	%fd467, %fd710, 0d3FF0000000000000;
	rcp.rn.f64 	%fd468, %fd467;
	add.f64 	%fd453, %fd710, 0dBFF0000000000000;
	mul.rn.f64 	%fd469, %fd453, %fd468;
	add.f64 	%fd458, %fd469, %fd469;
	mul.rn.f64 	%fd449, %fd458, %fd458;
	mov.f64 	%fd424, 0d3EB1380B3AE80F1E;
	mov.f64 	%fd426, 0d3ED0EE258B7A8B04;
	// inline asm
	fma.rn.f64 	%fd423, %fd424, %fd449, %fd426;
	// inline asm
	mov.f64 	%fd430, 0d3EF3B2669F02676F;
	// inline asm
	fma.rn.f64 	%fd427, %fd423, %fd449, %fd430;
	// inline asm
	mov.f64 	%fd434, 0d3F1745CBA9AB0956;
	// inline asm
	fma.rn.f64 	%fd431, %fd427, %fd449, %fd434;
	// inline asm
	mov.f64 	%fd438, 0d3F3C71C72D1B5154;
	// inline asm
	fma.rn.f64 	%fd435, %fd431, %fd449, %fd438;
	// inline asm
	mov.f64 	%fd442, 0d3F624924923BE72D;
	// inline asm
	fma.rn.f64 	%fd439, %fd435, %fd449, %fd442;
	// inline asm
	mov.f64 	%fd446, 0d3F8999999999A3C4;
	// inline asm
	fma.rn.f64 	%fd443, %fd439, %fd449, %fd446;
	// inline asm
	mov.f64 	%fd450, 0d3FB5555555555554;
	// inline asm
	fma.rn.f64 	%fd447, %fd443, %fd449, %fd450;
	// inline asm
	sub.f64 	%fd470, %fd453, %fd458;
	mov.f64 	%fd471, 0d4000000000000000;
	mul.rn.f64 	%fd454, %fd471, %fd470;
	neg.f64 	%fd452, %fd458;
	// inline asm
	fma.rn.f64 	%fd451, %fd452, %fd453, %fd454;
	// inline asm
	mul.rn.f64 	%fd472, %fd468, %fd451;
	mul.rn.f64 	%fd473, %fd447, %fd449;
	mul.rn.f64 	%fd474, %fd473, %fd458;
	add.f64 	%fd475, %fd472, %fd474;
	cvt.rn.f64.s32 	%fd464, %r571;
	mov.f64 	%fd461, 0d3FE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd455, %fd464, %fd461, %fd458;
	// inline asm
	neg.s32 	%r402, %r571;
	cvt.rn.f64.s32 	%fd460, %r402;
	// inline asm
	fma.rn.f64 	%fd459, %fd460, %fd461, %fd455;
	// inline asm
	sub.f64 	%fd476, %fd459, %fd458;
	sub.f64 	%fd466, %fd475, %fd476;
	mov.f64 	%fd465, 0d3C7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd463, %fd464, %fd465, %fd466;
	// inline asm
	add.f64 	%fd711, %fd455, %fd463;

BB0_102:
	mul.f64 	%fd478, %fd711, 0dC000000000000000;
	// inline asm
	sqrt.rn.f64 	%fd477, %fd478;
	// inline asm
	mul.f64 	%fd64, %fd54, 0d401921FB54442D18;
	setp.eq.f64 	%p72, %fd64, %fd2;
	setp.eq.f64 	%p73, %fd64, %fd41;
	or.pred  	%p74, %p72, %p73;
	@%p74 bra 	BB0_125;

	// inline asm
	abs.f64 	%fd479, %fd64;
	// inline asm
	setp.gt.f64 	%p75, %fd479, 0d41E0000000000000;
	@%p75 bra 	BB0_105;

	mov.f64 	%fd494, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd481, %fd64, %fd494;
	// inline asm
	cvt.rni.s32.f64 	%r403, %fd481;
	// inline asm
	cvt.rn.f64.s32 	%fd495, %r403;
	neg.f64 	%fd491, %fd495;
	mov.f64 	%fd484, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd482, %fd491, %fd484, %fd64;
	// inline asm
	mov.f64 	%fd488, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd486, %fd491, %fd488, %fd482;
	// inline asm
	mov.f64 	%fd492, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd490, %fd491, %fd492, %fd486;
	// inline asm
	mov.u32 	%r574, %r403;
	mov.f64 	%fd712, %fd490;
	bra.uni 	BB0_121;

BB0_105:
	mov.b64 	 %rl65, %fd64;
	and.b64  	%rl316, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl211, %rl67, 2047;
	add.s64 	%rl212, %rl211, 4294966272;
	cvt.u32.u64 	%r132, %rl212;
	shl.b64 	%rl213, %rl65, 11;
	or.b64  	%rl68, %rl213, -9223372036854775808;
	shr.u32 	%r407, %r132, 6;
	mov.u32 	%r408, 16;
	sub.s32 	%r133, %r408, %r407;
	mov.u32 	%r409, 15;
	sub.s32 	%r572, %r409, %r407;
	mov.u32 	%r410, 19;
	sub.s32 	%r135, %r410, %r407;
	mov.u32 	%r405, 18;
	// inline asm
	min.s32 	%r404, %r405, %r135;
	// inline asm
	setp.lt.s32 	%p76, %r572, %r404;
	@%p76 bra 	BB0_107;

	mov.u64 	%rl313, 0;
	bra.uni 	BB0_109;

BB0_107:
	mov.u32 	%r411, 1;
	sub.s32 	%r136, %r411, %r133;
	mov.u64 	%rl313, 0;

BB0_108:
	.pragma "nounroll";
	shl.b32 	%r415, %r572, 3;
	mov.u32 	%r416, __internal_i2opi_d;
	add.s32 	%r417, %r416, %r415;
	ld.const.u64 	%rl217, [%r417];
	mul.lo.s64 	%rl219, %rl217, %rl68;
	// inline asm
	mul.hi.u64 	%rl216, %rl217, %rl68;
	// inline asm
	mad.lo.s64 	%rl220, %rl217, %rl68, %rl313;
	setp.lt.u64 	%p77, %rl220, %rl219;
	selp.u64 	%rl221, 1, 0, %p77;
	add.s64 	%rl313, %rl221, %rl216;
	add.s32 	%r418, %r136, %r572;
	shl.b32 	%r419, %r418, 3;
	add.u32 	%r420, %SP, 40;
	add.s32 	%r421, %r420, %r419;
	st.local.u64 	[%r421], %rl220;
	// inline asm
	min.s32 	%r412, %r405, %r135;
	// inline asm
	add.s32 	%r572, %r572, 1;
	setp.lt.s32 	%p78, %r572, %r412;
	@%p78 bra 	BB0_108;

BB0_109:
	mov.u32 	%r422, 1;
	sub.s32 	%r423, %r422, %r133;
	add.s32 	%r424, %r423, %r572;
	shl.b32 	%r425, %r424, 3;
	add.u32 	%r426, %SP, 40;
	add.s32 	%r427, %r426, %r425;
	st.local.u64 	[%r427], %rl313;
	ld.local.u64 	%rl314, [%SP+64];
	ld.local.u64 	%rl315, [%SP+56];
	and.b32  	%r428, %r132, 63;
	setp.eq.s32 	%p79, %r428, 0;
	@%p79 bra 	BB0_111;

	and.b64  	%rl222, %rl67, 63;
	cvt.u32.u64 	%r429, %rl222;
	shl.b64 	%rl223, %rl314, %r429;
	neg.s32 	%r430, %r132;
	and.b32  	%r431, %r430, 63;
	shr.u64 	%rl224, %rl315, %r431;
	or.b64  	%rl314, %rl224, %rl223;
	shl.b64 	%rl225, %rl315, %r429;
	ld.local.u64 	%rl226, [%SP+48];
	shr.u64 	%rl227, %rl226, %r431;
	or.b64  	%rl315, %rl227, %rl225;

BB0_111:
	shr.u64 	%rl228, %rl314, 62;
	cvt.u32.u64 	%r432, %rl228;
	shr.u64 	%rl229, %rl315, 62;
	shl.b64 	%rl230, %rl314, 2;
	or.b64  	%rl320, %rl229, %rl230;
	shl.b64 	%rl79, %rl315, 2;
	setp.ne.s64 	%p80, %rl79, 0;
	selp.u64 	%rl231, 1, 0, %p80;
	or.b64  	%rl232, %rl231, %rl320;
	setp.gt.u64 	%p81, %rl232, -9223372036854775808;
	selp.u32 	%r433, 1, 0, %p81;
	add.s32 	%r434, %r433, %r432;
	neg.s32 	%r435, %r434;
	setp.lt.s64 	%p82, %rl65, 0;
	selp.b32 	%r574, %r435, %r434, %p82;
	@%p81 bra 	BB0_113;

	mov.u64 	%rl319, %rl79;
	bra.uni 	BB0_114;

BB0_113:
	not.b64 	%rl233, %rl320;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p83, %rl79, 0;
	selp.u64 	%rl234, 1, 0, %p83;
	add.s64 	%rl320, %rl234, %rl233;
	xor.b64  	%rl316, %rl316, -9223372036854775808;
	mov.u64 	%rl319, %rl80;

BB0_114:
	mov.u64 	%rl318, %rl319;
	setp.gt.s64 	%p84, %rl320, 0;
	@%p84 bra 	BB0_116;

	mov.u32 	%r573, 0;
	bra.uni 	BB0_118;

BB0_116:
	mov.u32 	%r573, 0;

BB0_117:
	shr.u64 	%rl235, %rl318, 63;
	shl.b64 	%rl236, %rl320, 1;
	or.b64  	%rl320, %rl235, %rl236;
	shl.b64 	%rl318, %rl318, 1;
	add.s32 	%r573, %r573, -1;
	setp.gt.s64 	%p85, %rl320, 0;
	@%p85 bra 	BB0_117;

BB0_118:
	mul.lo.s64 	%rl322, %rl320, -3958705157555305931;
	mov.u64 	%rl239, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl237, %rl320, %rl239;
	// inline asm
	setp.gt.s64 	%p86, %rl237, 0;
	mov.u64 	%rl321, %rl237;
	@%p86 bra 	BB0_119;
	bra.uni 	BB0_120;

BB0_119:
	shl.b64 	%rl240, %rl237, 1;
	shr.u64 	%rl241, %rl322, 63;
	or.b64  	%rl321, %rl240, %rl241;
	mul.lo.s64 	%rl322, %rl320, -7917410315110611862;
	add.s32 	%r573, %r573, -1;

BB0_120:
	setp.ne.s64 	%p87, %rl322, 0;
	selp.u64 	%rl242, 1, 0, %p87;
	add.s64 	%rl243, %rl242, %rl321;
	add.s32 	%r438, %r573, 1022;
	cvt.u64.u32 	%rl244, %r438;
	shl.b64 	%rl245, %rl244, 52;
	shr.u64 	%rl246, %rl243, 11;
	shr.u64 	%rl247, %rl243, 10;
	and.b64  	%rl248, %rl247, 1;
	add.s64 	%rl249, %rl245, %rl246;
	add.s64 	%rl250, %rl249, %rl248;
	or.b64  	%rl251, %rl250, %rl316;
	mov.b64 	 %fd712, %rl251;

BB0_121:
	add.s32 	%r147, %r574, 1;
	and.b32  	%r439, %r147, 1;
	setp.eq.s32 	%p88, %r439, 0;
	mul.rn.f64 	%fd68, %fd712, %fd712;
	@%p88 bra 	BB0_123;

	mov.f64 	%fd497, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd499, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd496, %fd497, %fd68, %fd499;
	// inline asm
	mov.f64 	%fd503, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd500, %fd496, %fd68, %fd503;
	// inline asm
	mov.f64 	%fd507, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd504, %fd500, %fd68, %fd507;
	// inline asm
	mov.f64 	%fd511, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd508, %fd504, %fd68, %fd511;
	// inline asm
	mov.f64 	%fd515, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd512, %fd508, %fd68, %fd515;
	// inline asm
	mov.f64 	%fd519, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd516, %fd512, %fd68, %fd519;
	// inline asm
	mov.f64 	%fd523, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd520, %fd516, %fd68, %fd523;
	// inline asm
	mov.f64 	%fd713, %fd520;
	bra.uni 	BB0_124;

BB0_123:
	mov.f64 	%fd525, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd527, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd524, %fd525, %fd68, %fd527;
	// inline asm
	mov.f64 	%fd531, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd528, %fd524, %fd68, %fd531;
	// inline asm
	mov.f64 	%fd535, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd532, %fd528, %fd68, %fd535;
	// inline asm
	mov.f64 	%fd539, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd536, %fd532, %fd68, %fd539;
	// inline asm
	mov.f64 	%fd543, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd540, %fd536, %fd68, %fd543;
	// inline asm
	mul.rn.f64 	%fd545, %fd540, %fd68;
	// inline asm
	fma.rn.f64 	%fd544, %fd545, %fd712, %fd712;
	// inline asm
	mov.f64 	%fd713, %fd544;

BB0_124:
	and.b32  	%r440, %r147, 2;
	setp.eq.s32 	%p89, %r440, 0;
	neg.f64 	%fd548, %fd713;
	selp.f64 	%fd714, %fd713, %fd548, %p89;
	bra.uni 	BB0_126;

BB0_125:
	mov.f64 	%fd714, 0dFFF8000000000000;

BB0_126:
	mul.f64 	%fd549, %fd477, %fd714;
	cvt.rn.f32.f64 	%f165, %fd549;
	mul.f32 	%f162, %f76, %f70;
	// inline asm
	sqrt.approx.f32 	%f161, %f162;
	// inline asm
	rcp.approx.f32 	%f166, %f161;
	mul.f32 	%f167, %f76, %f71;
	mul.f32 	%f168, %f166, %f167;
	neg.f32 	%f169, %f53;
	fma.rn.f32 	%f170, %f76, %f72, %f169;
	neg.f32 	%f171, %f168;
	fma.rn.f32 	%f164, %f171, %f168, %f170;
	// inline asm
	sqrt.approx.f32 	%f163, %f164;
	// inline asm
	fma.rn.f32 	%f172, %f161, %f77, %f73;
	fma.rn.f32 	%f260, %f165, 0f00000000, %f172;
	fma.rn.f32 	%f173, %f77, 0f00000000, %f74;
	fma.rn.f32 	%f259, %f163, %f165, %f173;
	setp.gt.s32 	%p90, %r551, %r40;
	@%p90 bra 	BB0_127;
	bra.uni 	BB0_128;

BB0_127:
	setp.gt.f32 	%p91, %f260, 0f00000000;
	selp.u32 	%r441, 1, 0, %p91;
	add.s32 	%r590, %r441, %r590;
	setp.gt.f32 	%p92, %f259, 0f00000000;
	selp.u32 	%r442, 1, 0, %p92;
	add.s32 	%r589, %r442, %r589;
	setp.lt.f32 	%p93, %f260, 0f00000000;
	selp.u32 	%r443, 1, 0, %p93;
	add.s32 	%r588, %r443, %r588;
	setp.lt.f32 	%p94, %f259, 0f00000000;
	selp.u32 	%r444, 1, 0, %p94;
	add.s32 	%r587, %r444, %r587;
	sub.f32 	%f174, %f260, %f259;
	setp.gt.f32 	%p95, %f174, 0f00000000;
	selp.u32 	%r445, 1, 0, %p95;
	add.s32 	%r586, %r445, %r586;
	sub.f32 	%f175, %f259, %f260;
	setp.gt.f32 	%p96, %f175, 0f00000000;
	selp.u32 	%r446, 1, 0, %p96;
	add.s32 	%r585, %r446, %r585;

BB0_128:
	ld.param.u32 	%r538, [CalculateStatisticalMapsGLMBayesian_param_14];
	setp.gt.s32 	%p97, %r538, 1;
	@%p97 bra 	BB0_130;

	mov.f32 	%f257, 0f00000000;
	mov.f32 	%f256, %f257;
	bra.uni 	BB0_132;

BB0_130:
	ld.param.u32 	%r526, [CalculateStatisticalMapsGLMBayesian_param_11];
	ld.param.u32 	%r531, [CalculateStatisticalMapsGLMBayesian_param_12];
	mul.lo.s32 	%r448, %r531, %r526;
	shl.b32 	%r160, %r448, 2;
	ld.param.u32 	%r537, [CalculateStatisticalMapsGLMBayesian_param_14];
	shl.b32 	%r449, %r537, 2;
	add.s32 	%r161, %r449, 4;
	mov.f32 	%f257, 0f00000000;
	mov.f32 	%f256, %f257;
	mov.f32 	%f255, %f257;
	mov.u32 	%r577, 1;
	ld.param.u32 	%r575, [CalculateStatisticalMapsGLMBayesian_param_6];
	mov.u32 	%r576, %r53;

BB0_131:
	mov.f32 	%f80, %f255;
	mov.u32 	%r163, %r576;
	mov.u32 	%r162, %r575;
	add.s32 	%r166, %r162, 4;
	ld.const.f32 	%f181, [%r162+4];
	ld.global.f32 	%f182, [%r163];
	neg.f32 	%f183, %f181;
	fma.rn.f32 	%f184, %f183, %f260, %f182;
	add.s32 	%r450, %r162, %r161;
	ld.const.f32 	%f185, [%r450];
	neg.f32 	%f186, %f185;
	fma.rn.f32 	%f83, %f186, %f259, %f184;
	fma.rn.f32 	%f257, %f83, %f83, %f257;
	fma.rn.f32 	%f256, %f83, %f80, %f256;
	add.s32 	%r165, %r163, %r160;
	add.s32 	%r577, %r577, 1;
	ld.param.u32 	%r536, [CalculateStatisticalMapsGLMBayesian_param_14];
	setp.lt.s32 	%p98, %r577, %r536;
	mov.u32 	%r575, %r166;
	mov.u32 	%r576, %r165;
	mov.f32 	%f255, %f83;
	@%p98 bra 	BB0_131;

BB0_132:
	div.full.f32 	%f187, %f257, %f76;
	add.f32 	%f188, %f1, %f187;
	rcp.approx.f32 	%f88, %f188;
	mul.f32 	%f189, %f88, %f256;
	div.full.f32 	%f258, %f189, %f76;
	mul.f64 	%fd556, %fd53, 0d40D069C000000000;
	mul.f64 	%fd551, %fd556, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd550, %fd551;
	// inline asm
	neg.f64 	%fd558, %fd550;
	fma.rn.f64 	%fd559, %fd558, %fd97, %fd556;
	cvt.rzi.s32.f64 	%r453, %fd559;
	cvt.rn.f64.s32 	%fd560, %r453;
	mul.f64 	%fd555, %fd560, %fd1;
	mul.f64 	%fd561, %fd560, 0d40D069C000000000;
	mul.f64 	%fd553, %fd561, %fd1;
	// inline asm
	cvt.rmi.f64.f64 	%fd552, %fd553;
	// inline asm
	neg.f64 	%fd562, %fd552;
	fma.rn.f64 	%fd563, %fd562, %fd97, %fd561;
	cvt.rzi.s32.f64 	%r560, %fd563;
	cvt.rn.f64.s32 	%fd564, %r560;
	mul.f64 	%fd76, %fd564, %fd1;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r451}, %fd555; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r452, hi}, %fd555; 
	}
	// inline asm
	setp.gt.f64 	%p99, %fd555, 0d0000000000000000;
	setp.lt.f64 	%p100, %fd555, %fd2;
	and.pred  	%p101, %p99, %p100;
	mov.f64 	%fd716, %fd555;
	mov.u32 	%r579, %r452;
	mov.u32 	%r578, %r451;
	@%p101 bra 	BB0_139;

	setp.nan.f64 	%p102, %fd555, %fd555;
	@%p102 bra 	BB0_138;

	setp.eq.f64 	%p103, %fd555, 0d0000000000000000;
	@%p103 bra 	BB0_137;

	setp.eq.f64 	%p104, %fd555, %fd2;
	@%p104 bra 	BB0_145;

	mov.f64 	%fd716, 0dFFF8000000000000;
	bra.uni 	BB0_145;

BB0_137:
	neg.f64 	%fd716, %fd2;
	bra.uni 	BB0_145;

BB0_138:
	add.f64 	%fd716, %fd555, %fd555;
	bra.uni 	BB0_145;

BB0_139:
	setp.lt.u32 	%p105, %r451, 1048576;
	@%p105 bra 	BB0_141;

	mov.u32 	%r580, -1023;
	bra.uni 	BB0_142;

BB0_141:
	mov.f64 	%fd567, 0d4350000000000000;
	mul.rn.f64 	%fd566, %fd555, %fd567;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r455}, %fd566; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r456, hi}, %fd566; 
	}
	// inline asm
	mov.u32 	%r580, -1077;
	mov.u32 	%r579, %r456;
	mov.u32 	%r578, %r455;

BB0_142:
	shr.s32 	%r460, %r578, 20;
	add.s32 	%r581, %r580, %r460;
	and.b32  	%r461, %r578, -2146435073;
	or.b32  	%r459, %r461, 1072693248;
	// inline asm
	mov.b64 	%fd568, {%r579, %r459};
	// inline asm
	setp.gt.u32 	%p106, %r459, 1073127582;
	mov.f64 	%fd715, %fd568;
	@%p106 bra 	BB0_143;
	bra.uni 	BB0_144;

BB0_143:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r462, hi}, %fd568; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r463}, %fd568; 
	}
	// inline asm
	add.s32 	%r465, %r463, -1048576;
	// inline asm
	mov.b64 	%fd571, {%r462, %r465};
	// inline asm
	add.s32 	%r581, %r581, 1;
	mov.f64 	%fd715, %fd571;

BB0_144:
	add.f64 	%fd616, %fd715, 0d3FF0000000000000;
	rcp.rn.f64 	%fd617, %fd616;
	add.f64 	%fd602, %fd715, 0dBFF0000000000000;
	mul.rn.f64 	%fd618, %fd602, %fd617;
	add.f64 	%fd607, %fd618, %fd618;
	mul.rn.f64 	%fd598, %fd607, %fd607;
	mov.f64 	%fd573, 0d3EB1380B3AE80F1E;
	mov.f64 	%fd575, 0d3ED0EE258B7A8B04;
	// inline asm
	fma.rn.f64 	%fd572, %fd573, %fd598, %fd575;
	// inline asm
	mov.f64 	%fd579, 0d3EF3B2669F02676F;
	// inline asm
	fma.rn.f64 	%fd576, %fd572, %fd598, %fd579;
	// inline asm
	mov.f64 	%fd583, 0d3F1745CBA9AB0956;
	// inline asm
	fma.rn.f64 	%fd580, %fd576, %fd598, %fd583;
	// inline asm
	mov.f64 	%fd587, 0d3F3C71C72D1B5154;
	// inline asm
	fma.rn.f64 	%fd584, %fd580, %fd598, %fd587;
	// inline asm
	mov.f64 	%fd591, 0d3F624924923BE72D;
	// inline asm
	fma.rn.f64 	%fd588, %fd584, %fd598, %fd591;
	// inline asm
	mov.f64 	%fd595, 0d3F8999999999A3C4;
	// inline asm
	fma.rn.f64 	%fd592, %fd588, %fd598, %fd595;
	// inline asm
	mov.f64 	%fd599, 0d3FB5555555555554;
	// inline asm
	fma.rn.f64 	%fd596, %fd592, %fd598, %fd599;
	// inline asm
	sub.f64 	%fd619, %fd602, %fd607;
	mov.f64 	%fd620, 0d4000000000000000;
	mul.rn.f64 	%fd603, %fd620, %fd619;
	neg.f64 	%fd601, %fd607;
	// inline asm
	fma.rn.f64 	%fd600, %fd601, %fd602, %fd603;
	// inline asm
	mul.rn.f64 	%fd621, %fd617, %fd600;
	mul.rn.f64 	%fd622, %fd596, %fd598;
	mul.rn.f64 	%fd623, %fd622, %fd607;
	add.f64 	%fd624, %fd621, %fd623;
	cvt.rn.f64.s32 	%fd613, %r581;
	mov.f64 	%fd610, 0d3FE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd604, %fd613, %fd610, %fd607;
	// inline asm
	neg.s32 	%r466, %r581;
	cvt.rn.f64.s32 	%fd609, %r466;
	// inline asm
	fma.rn.f64 	%fd608, %fd609, %fd610, %fd604;
	// inline asm
	sub.f64 	%fd625, %fd608, %fd607;
	sub.f64 	%fd615, %fd624, %fd625;
	mov.f64 	%fd614, 0d3C7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd612, %fd613, %fd614, %fd615;
	// inline asm
	add.f64 	%fd716, %fd604, %fd612;

BB0_145:
	mul.f64 	%fd627, %fd716, 0dC000000000000000;
	// inline asm
	sqrt.rn.f64 	%fd626, %fd627;
	// inline asm
	mul.f64 	%fd86, %fd76, 0d401921FB54442D18;
	setp.eq.f64 	%p107, %fd86, %fd2;
	setp.eq.f64 	%p108, %fd86, %fd41;
	or.pred  	%p109, %p107, %p108;
	@%p109 bra 	BB0_168;

	// inline asm
	abs.f64 	%fd628, %fd86;
	// inline asm
	setp.gt.f64 	%p110, %fd628, 0d41E0000000000000;
	@%p110 bra 	BB0_148;

	mov.f64 	%fd643, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd630, %fd86, %fd643;
	// inline asm
	cvt.rni.s32.f64 	%r467, %fd630;
	// inline asm
	cvt.rn.f64.s32 	%fd644, %r467;
	neg.f64 	%fd640, %fd644;
	mov.f64 	%fd633, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd631, %fd640, %fd633, %fd86;
	// inline asm
	mov.f64 	%fd637, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd635, %fd640, %fd637, %fd631;
	// inline asm
	mov.f64 	%fd641, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd639, %fd640, %fd641, %fd635;
	// inline asm
	mov.u32 	%r584, %r467;
	mov.f64 	%fd717, %fd639;
	bra.uni 	BB0_164;

BB0_148:
	mov.b64 	 %rl97, %fd86;
	and.b64  	%rl326, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl252, %rl99, 2047;
	add.s64 	%rl253, %rl252, 4294966272;
	cvt.u32.u64 	%r180, %rl253;
	shl.b64 	%rl254, %rl97, 11;
	or.b64  	%rl100, %rl254, -9223372036854775808;
	shr.u32 	%r471, %r180, 6;
	mov.u32 	%r472, 16;
	sub.s32 	%r181, %r472, %r471;
	mov.u32 	%r473, 15;
	sub.s32 	%r582, %r473, %r471;
	mov.u32 	%r474, 19;
	sub.s32 	%r183, %r474, %r471;
	mov.u32 	%r469, 18;
	// inline asm
	min.s32 	%r468, %r469, %r183;
	// inline asm
	setp.lt.s32 	%p111, %r582, %r468;
	@%p111 bra 	BB0_150;

	mov.u64 	%rl323, 0;
	bra.uni 	BB0_152;

BB0_150:
	mov.u32 	%r475, 1;
	sub.s32 	%r184, %r475, %r181;
	mov.u64 	%rl323, 0;

BB0_151:
	.pragma "nounroll";
	shl.b32 	%r479, %r582, 3;
	mov.u32 	%r480, __internal_i2opi_d;
	add.s32 	%r481, %r480, %r479;
	ld.const.u64 	%rl258, [%r481];
	mul.lo.s64 	%rl260, %rl258, %rl100;
	// inline asm
	mul.hi.u64 	%rl257, %rl258, %rl100;
	// inline asm
	mad.lo.s64 	%rl261, %rl258, %rl100, %rl323;
	setp.lt.u64 	%p112, %rl261, %rl260;
	selp.u64 	%rl262, 1, 0, %p112;
	add.s64 	%rl323, %rl262, %rl257;
	add.s32 	%r482, %r184, %r582;
	shl.b32 	%r483, %r482, 3;
	add.u32 	%r484, %SP, 40;
	add.s32 	%r485, %r484, %r483;
	st.local.u64 	[%r485], %rl261;
	// inline asm
	min.s32 	%r476, %r469, %r183;
	// inline asm
	add.s32 	%r582, %r582, 1;
	setp.lt.s32 	%p113, %r582, %r476;
	@%p113 bra 	BB0_151;

BB0_152:
	mov.u32 	%r486, 1;
	sub.s32 	%r487, %r486, %r181;
	add.s32 	%r488, %r487, %r582;
	shl.b32 	%r489, %r488, 3;
	add.u32 	%r490, %SP, 40;
	add.s32 	%r491, %r490, %r489;
	st.local.u64 	[%r491], %rl323;
	ld.local.u64 	%rl324, [%SP+64];
	ld.local.u64 	%rl325, [%SP+56];
	and.b32  	%r492, %r180, 63;
	setp.eq.s32 	%p114, %r492, 0;
	@%p114 bra 	BB0_154;

	and.b64  	%rl263, %rl99, 63;
	cvt.u32.u64 	%r493, %rl263;
	shl.b64 	%rl264, %rl324, %r493;
	neg.s32 	%r494, %r180;
	and.b32  	%r495, %r494, 63;
	shr.u64 	%rl265, %rl325, %r495;
	or.b64  	%rl324, %rl265, %rl264;
	shl.b64 	%rl266, %rl325, %r493;
	ld.local.u64 	%rl267, [%SP+48];
	shr.u64 	%rl268, %rl267, %r495;
	or.b64  	%rl325, %rl268, %rl266;

BB0_154:
	shr.u64 	%rl269, %rl324, 62;
	cvt.u32.u64 	%r496, %rl269;
	shr.u64 	%rl270, %rl325, 62;
	shl.b64 	%rl271, %rl324, 2;
	or.b64  	%rl330, %rl270, %rl271;
	shl.b64 	%rl111, %rl325, 2;
	setp.ne.s64 	%p115, %rl111, 0;
	selp.u64 	%rl272, 1, 0, %p115;
	or.b64  	%rl273, %rl272, %rl330;
	setp.gt.u64 	%p116, %rl273, -9223372036854775808;
	selp.u32 	%r497, 1, 0, %p116;
	add.s32 	%r498, %r497, %r496;
	neg.s32 	%r499, %r498;
	setp.lt.s64 	%p117, %rl97, 0;
	selp.b32 	%r584, %r499, %r498, %p117;
	@%p116 bra 	BB0_156;

	mov.u64 	%rl329, %rl111;
	bra.uni 	BB0_157;

BB0_156:
	not.b64 	%rl274, %rl330;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p118, %rl111, 0;
	selp.u64 	%rl275, 1, 0, %p118;
	add.s64 	%rl330, %rl275, %rl274;
	xor.b64  	%rl326, %rl326, -9223372036854775808;
	mov.u64 	%rl329, %rl112;

BB0_157:
	mov.u64 	%rl328, %rl329;
	setp.gt.s64 	%p119, %rl330, 0;
	@%p119 bra 	BB0_159;

	mov.u32 	%r583, 0;
	bra.uni 	BB0_161;

BB0_159:
	mov.u32 	%r583, 0;

BB0_160:
	shr.u64 	%rl276, %rl328, 63;
	shl.b64 	%rl277, %rl330, 1;
	or.b64  	%rl330, %rl276, %rl277;
	shl.b64 	%rl328, %rl328, 1;
	add.s32 	%r583, %r583, -1;
	setp.gt.s64 	%p120, %rl330, 0;
	@%p120 bra 	BB0_160;

BB0_161:
	mul.lo.s64 	%rl332, %rl330, -3958705157555305931;
	mov.u64 	%rl280, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl278, %rl330, %rl280;
	// inline asm
	setp.gt.s64 	%p121, %rl278, 0;
	mov.u64 	%rl331, %rl278;
	@%p121 bra 	BB0_162;
	bra.uni 	BB0_163;

BB0_162:
	shl.b64 	%rl281, %rl278, 1;
	shr.u64 	%rl282, %rl332, 63;
	or.b64  	%rl331, %rl281, %rl282;
	mul.lo.s64 	%rl332, %rl330, -7917410315110611862;
	add.s32 	%r583, %r583, -1;

BB0_163:
	setp.ne.s64 	%p122, %rl332, 0;
	selp.u64 	%rl283, 1, 0, %p122;
	add.s64 	%rl284, %rl283, %rl331;
	add.s32 	%r502, %r583, 1022;
	cvt.u64.u32 	%rl285, %r502;
	shl.b64 	%rl286, %rl285, 52;
	shr.u64 	%rl287, %rl284, 11;
	shr.u64 	%rl288, %rl284, 10;
	and.b64  	%rl289, %rl288, 1;
	add.s64 	%rl290, %rl286, %rl287;
	add.s64 	%rl291, %rl290, %rl289;
	or.b64  	%rl292, %rl291, %rl326;
	mov.b64 	 %fd717, %rl292;

BB0_164:
	add.s32 	%r195, %r584, 1;
	and.b32  	%r503, %r195, 1;
	setp.eq.s32 	%p123, %r503, 0;
	mul.rn.f64 	%fd90, %fd717, %fd717;
	@%p123 bra 	BB0_166;

	mov.f64 	%fd646, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd648, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd645, %fd646, %fd90, %fd648;
	// inline asm
	mov.f64 	%fd652, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd649, %fd645, %fd90, %fd652;
	// inline asm
	mov.f64 	%fd656, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd653, %fd649, %fd90, %fd656;
	// inline asm
	mov.f64 	%fd660, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd657, %fd653, %fd90, %fd660;
	// inline asm
	mov.f64 	%fd664, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd661, %fd657, %fd90, %fd664;
	// inline asm
	mov.f64 	%fd668, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd665, %fd661, %fd90, %fd668;
	// inline asm
	mov.f64 	%fd672, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd669, %fd665, %fd90, %fd672;
	// inline asm
	mov.f64 	%fd718, %fd669;
	bra.uni 	BB0_167;

BB0_166:
	mov.f64 	%fd674, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd676, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd673, %fd674, %fd90, %fd676;
	// inline asm
	mov.f64 	%fd680, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd677, %fd673, %fd90, %fd680;
	// inline asm
	mov.f64 	%fd684, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd681, %fd677, %fd90, %fd684;
	// inline asm
	mov.f64 	%fd688, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd685, %fd681, %fd90, %fd688;
	// inline asm
	mov.f64 	%fd692, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd689, %fd685, %fd90, %fd692;
	// inline asm
	mul.rn.f64 	%fd694, %fd689, %fd90;
	// inline asm
	fma.rn.f64 	%fd693, %fd694, %fd717, %fd717;
	// inline asm
	mov.f64 	%fd718, %fd693;

BB0_167:
	and.b32  	%r504, %r195, 2;
	setp.eq.s32 	%p124, %r504, 0;
	neg.f64 	%fd697, %fd718;
	selp.f64 	%fd719, %fd718, %fd697, %p124;
	bra.uni 	BB0_169;

BB0_168:
	mov.f64 	%fd719, 0dFFF8000000000000;

BB0_169:
	mul.f64 	%fd698, %fd626, %fd719;
	cvt.rn.f32.f64 	%f192, %fd698;
	mul.f32 	%f191, %f76, %f88;
	// inline asm
	sqrt.approx.f32 	%f190, %f191;
	// inline asm
	fma.rn.f32 	%f193, %f190, %f192, %f258;
	setp.lt.f32 	%p125, %f193, 0f00000000;
	neg.f32 	%f194, %f193;
	selp.f32 	%f195, %f194, %f193, %p125;
	setp.lt.f32 	%p126, %f195, 0f3F800000;
	selp.f32 	%f232, %f193, %f232, %p126;
	add.f32 	%f196, %f232, %f232;
	ld.param.u32 	%r522, [CalculateStatisticalMapsGLMBayesian_param_9];
	ld.const.f32 	%f197, [%r522];
	ld.param.u32 	%r520, [CalculateStatisticalMapsGLMBayesian_param_8];
	ld.const.f32 	%f198, [%r520];
	neg.f32 	%f199, %f196;
	fma.rn.f32 	%f200, %f199, %f197, %f198;
	mul.f32 	%f201, %f232, %f232;
	ld.param.u32 	%r524, [CalculateStatisticalMapsGLMBayesian_param_10];
	ld.const.f32 	%f202, [%r524];
	fma.rn.f32 	%f91, %f201, %f202, %f200;
	fma.rn.f32 	%f203, %f199, %f54, %f46;
	fma.rn.f32 	%f92, %f201, %f55, %f203;
	fma.rn.f32 	%f204, %f199, %f56, %f47;
	fma.rn.f32 	%f93, %f201, %f57, %f204;
	fma.rn.f32 	%f205, %f199, %f58, %f48;
	fma.rn.f32 	%f94, %f201, %f59, %f205;
	neg.f32 	%f206, %f232;
	fma.rn.f32 	%f207, %f206, %f60, %f44;
	fma.rn.f32 	%f95, %f201, %f227, %f207;
	fma.rn.f32 	%f208, %f206, %f61, %f43;
	fma.rn.f32 	%f96, %f201, %f226, %f208;
	fma.rn.f32 	%f209, %f199, %f225, %f36;
	fma.rn.f32 	%f97, %f201, %f224, %f209;
	ld.param.u32 	%r544, [CalculateStatisticalMapsGLMBayesian_param_16];
	add.s32 	%r505, %r40, %r544;
	add.s32 	%r551, %r551, 1;
	setp.lt.s32 	%p127, %r551, %r505;
	mov.f32 	%f236, %f97;
	mov.f32 	%f242, %f96;
	mov.f32 	%f248, %f95;
	mov.f32 	%f251, %f94;
	mov.f32 	%f252, %f93;
	mov.f32 	%f253, %f92;
	mov.f32 	%f254, %f91;
	@%p127 bra 	BB0_11;

BB0_170:
	cvt.rn.f32.s32 	%f210, %r590;
	div.full.f32 	%f211, %f210, %f2;
	st.global.f32 	[%r30], %f211;
	cvt.rn.f32.s32 	%f212, %r589;
	div.full.f32 	%f213, %f212, %f2;
	st.global.f32 	[%r31], %f213;
	cvt.rn.f32.s32 	%f214, %r588;
	div.full.f32 	%f215, %f214, %f2;
	st.global.f32 	[%r32], %f215;
	cvt.rn.f32.s32 	%f216, %r587;
	div.full.f32 	%f217, %f216, %f2;
	st.global.f32 	[%r33], %f217;
	cvt.rn.f32.s32 	%f218, %r586;
	div.full.f32 	%f219, %f218, %f2;
	st.global.f32 	[%r34], %f219;
	cvt.rn.f32.s32 	%f220, %r585;
	div.full.f32 	%f221, %f220, %f2;
	st.global.f32 	[%r35], %f221;
	st.global.f32 	[%r36], %f260;
	st.global.f32 	[%r37], %f259;
	st.global.f32 	[%r38], %f258;
	ret;

BB0_171:
	mov.u32 	%r506, 0;
	st.global.u32 	[%r30], %r506;
	st.global.u32 	[%r31], %r506;
	st.global.u32 	[%r32], %r506;
	st.global.u32 	[%r33], %r506;
	st.global.u32 	[%r34], %r506;
	st.global.u32 	[%r35], %r506;
	st.global.u32 	[%r36], %r506;
	st.global.u32 	[%r37], %r506;
	st.global.u32 	[%r38], %r506;
	ret;
}


